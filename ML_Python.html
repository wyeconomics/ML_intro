<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Wooyong Park">
<meta name="dcterms.date" content="2025-02-16">

<title>Machine Learning with Python</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="ML_Python_files/libs/clipboard/clipboard.min.js"></script>
<script src="ML_Python_files/libs/quarto-html/quarto.js"></script>
<script src="ML_Python_files/libs/quarto-html/popper.min.js"></script>
<script src="ML_Python_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="ML_Python_files/libs/quarto-html/anchor.min.js"></script>
<link href="ML_Python_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="ML_Python_files/libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="ML_Python_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="ML_Python_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="ML_Python_files/libs/bootstrap/bootstrap-973236bd072d72a04ee9cd82dcc9cb29.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#what-is-machine-learning" id="toc-what-is-machine-learning" class="nav-link active" data-scroll-target="#what-is-machine-learning">What is Machine Learning?</a></li>
  <li><a href="#supervised-learning" id="toc-supervised-learning" class="nav-link" data-scroll-target="#supervised-learning">Supervised Learning</a>
  <ul class="collapse">
  <li><a href="#example-workflow" id="toc-example-workflow" class="nav-link" data-scroll-target="#example-workflow">Example Workflow</a></li>
  </ul></li>
  <li><a href="#classification" id="toc-classification" class="nav-link" data-scroll-target="#classification">Classification</a>
  <ul class="collapse">
  <li><a href="#k-nearest-neighborsknn" id="toc-k-nearest-neighborsknn" class="nav-link" data-scroll-target="#k-nearest-neighborsknn">k-Nearest Neighbors(KNN)</a></li>
  <li><a href="#sec-tree" id="toc-sec-tree" class="nav-link" data-scroll-target="#sec-tree">Tree-based Models(Decision Tree and Random Forests)</a></li>
  <li><a href="#logistic-regressions" id="toc-logistic-regressions" class="nav-link" data-scroll-target="#logistic-regressions">Logistic Regressions</a></li>
  </ul></li>
  <li><a href="#predictions" id="toc-predictions" class="nav-link" data-scroll-target="#predictions">Predictions</a>
  <ul class="collapse">
  <li><a href="#sec-univ_reg" id="toc-sec-univ_reg" class="nav-link" data-scroll-target="#sec-univ_reg">Univariate Regression</a></li>
  <li><a href="#multinomial-regression-with-polynomialfeatures" id="toc-multinomial-regression-with-polynomialfeatures" class="nav-link" data-scroll-target="#multinomial-regression-with-polynomialfeatures">Multinomial Regression with <code>PolynomialFeatures</code></a></li>
  <li><a href="#multivariate-regressions" id="toc-multivariate-regressions" class="nav-link" data-scroll-target="#multivariate-regressions">Multivariate Regressions</a></li>
  <li><a href="#penalized-regressions" id="toc-penalized-regressions" class="nav-link" data-scroll-target="#penalized-regressions">Penalized Regressions</a></li>
  </ul></li>
  <li><a href="#sec-model_performance" id="toc-sec-model_performance" class="nav-link" data-scroll-target="#sec-model_performance">Methods of Measuring Model Performance</a>
  <ul class="collapse">
  <li><a href="#confusion-matrix" id="toc-confusion-matrix" class="nav-link" data-scroll-target="#confusion-matrix">Confusion Matrix</a></li>
  <li><a href="#accuracy" id="toc-accuracy" class="nav-link" data-scroll-target="#accuracy">Accuracy</a></li>
  <li><a href="#sensitivity-and-specificity" id="toc-sensitivity-and-specificity" class="nav-link" data-scroll-target="#sensitivity-and-specificity">Sensitivity and Specificity</a></li>
  <li><a href="#precision-and-negative-predictive" id="toc-precision-and-negative-predictive" class="nav-link" data-scroll-target="#precision-and-negative-predictive">Precision and Negative Predictive</a></li>
  <li><a href="#f1-score" id="toc-f1-score" class="nav-link" data-scroll-target="#f1-score">F1 Score</a></li>
  <li><a href="#r-squared-and-mse" id="toc-r-squared-and-mse" class="nav-link" data-scroll-target="#r-squared-and-mse">R-Squared and MSE</a></li>
  </ul></li>
  <li><a href="#cross-validation-and-hyperparameter-tuning" id="toc-cross-validation-and-hyperparameter-tuning" class="nav-link" data-scroll-target="#cross-validation-and-hyperparameter-tuning">Cross Validation and Hyperparameter Tuning</a>
  <ul class="collapse">
  <li><a href="#why-cross-validation-for-hyperparameter-tuning" id="toc-why-cross-validation-for-hyperparameter-tuning" class="nav-link" data-scroll-target="#why-cross-validation-for-hyperparameter-tuning">Why cross-validation for hyperparameter tuning?</a></li>
  <li><a href="#grid-search-cross-validation" id="toc-grid-search-cross-validation" class="nav-link" data-scroll-target="#grid-search-cross-validation">Grid Search cross-validation</a></li>
  <li><a href="#randomized-search-cross-validation" id="toc-randomized-search-cross-validation" class="nav-link" data-scroll-target="#randomized-search-cross-validation">Randomized Search cross-validation</a></li>
  </ul></li>
  <li><a href="#pipeline-operators" id="toc-pipeline-operators" class="nav-link" data-scroll-target="#pipeline-operators">Pipeline Operators</a></li>
  <li><a href="#scikit-learn-cheatsheets" id="toc-scikit-learn-cheatsheets" class="nav-link" data-scroll-target="#scikit-learn-cheatsheets"><code>scikit-learn</code> Cheatsheets</a>
  <ul class="collapse">
  <li><a href="#handling-missing-data" id="toc-handling-missing-data" class="nav-link" data-scroll-target="#handling-missing-data">Handling Missing Data</a></li>
  </ul></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix">Appendix</a>
  <ul class="collapse">
  <li><a href="#sec-knn_algorithm" id="toc-sec-knn_algorithm" class="nav-link" data-scroll-target="#sec-knn_algorithm">A.1. KNN Algorithm</a></li>
  <li><a href="#sec-rpart" id="toc-sec-rpart" class="nav-link" data-scroll-target="#sec-rpart">A.2. Recursive Partitioning</a></li>
  <li><a href="#a.3.-lasso-vs-ols" id="toc-a.3.-lasso-vs-ols" class="nav-link" data-scroll-target="#a.3.-lasso-vs-ols">A.3. LASSO vs OLS</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Machine Learning with Python</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Wooyong Park </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 16, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="what-is-machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="what-is-machine-learning">What is Machine Learning?</h2>
<p>Machine learning(ML) is the process of computer learning the data to discover the relation of multiple variables and make decisions based on them. These decisions could either be prediction, classification, or clustering. Specifically, prediction and classification are grouped into <strong><em>supervised learning</em></strong>, and clustering is also named <strong><em>unsupervised learning</em></strong>.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Supervised Learning</th>
<th style="text-align: left;">Unsupervised Learning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Algorithm</td>
<td style="text-align: left;">Regression and Classification</td>
<td style="text-align: left;">Clustering, Association, and Anomaly Detection</td>
</tr>
<tr class="even">
<td style="text-align: left;">Characteristics</td>
<td style="text-align: left;">- Correct answers are given to the computer<br>- Diverse ways to test/score models</td>
<td style="text-align: left;">- Answers not given to the computer<br> - Limited ways to test/score models</td>
</tr>
</tbody>
</table>
</section>
<section id="supervised-learning" class="level2">
<h2 class="anchored" data-anchor-id="supervised-learning">Supervised Learning</h2>
<p>Supervised learning is classified as prediction or classification, according to whether the “target variable” is continuous or discrete. We aim to predict/classify the target variable’s value based on the values of “predictor variables”.</p>
<section id="example-workflow" class="level3">
<h3 class="anchored" data-anchor-id="example-workflow">Example Workflow</h3>
<div id="074bce09" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.module <span class="im">import</span> Model   <span class="co">#import the type of algorithm</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model() <span class="co">#instantiate the model</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)  <span class="co">#X: predictor variables, y: target variable (from training data)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> model.predict(X_new) <span class="co">#X_new: the new data(from test data)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that in the example workflow above, <code>module</code>, <code>Model</code>, <code>X</code>, <code>y</code>, <code>model.predict</code>, and <code>X_new</code> are values to be modified for actual use.</p>
</section>
</section>
<section id="classification" class="level2">
<h2 class="anchored" data-anchor-id="classification">Classification</h2>
<p>Classification is the process of predicting a label for the observation. Labels are discrete variable values that classifies each observation into different group. Specific algorithms include KNN, SVM, decision trees, and logistic regressions.</p>
<section id="k-nearest-neighborsknn" class="level3">
<h3 class="anchored" data-anchor-id="k-nearest-neighborsknn">k-Nearest Neighbors(KNN)</h3>
<p><strong>k-Nearest Neighbors</strong>, more often called as <strong>KNN</strong> predicts the label by looking at the <code>k</code> closest labelled data points, and take the majority value of label as the observation’s label value.</p>
<div id="c396b851" class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="ML_Python_files/figure-html/cell-3-output-1.png" width="521" height="376" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The natural question following would be “How should I choose the right <code>k</code>?” A smaller <code>k</code> would mean that individual data gains more power in the classification, which would turn more complex models and overfitting. Likewise, if <code>k</code> is too large, it would give too simple models and cause underfitting. Hence, the question also faces the <strong>bias-variance tradeoff</strong> and researchers should be careful not to arrive at both extremes. For a theoretical background for KNN, please go to <a href="./#sec-knn_algorithm">A.1.</a></p>
<div id="b6dbb841" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="ML_Python_files/figure-html/cell-4-output-1.png" width="1430" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Let’s begin practicing <strong>KNN</strong> with the following example workflow. In this workflow, we will use the <code>titanic</code> data from the <code>seaborn</code> package and predict whether a person survived or not with the predictor variables <code>pclass</code>, <code>sex</code>, <code>age</code>, <code>sibsp</code>, <code>parch</code>, and <code>embarked</code>. First, load the dataset.</p>
<div id="5db7636a" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> sns.load_dataset(<span class="st">'titanic'</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.head())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \
0         0       3    male  22.0      1      0   7.2500        S  Third   
1         1       1  female  38.0      1      0  71.2833        C  First   
2         1       3  female  26.0      0      0   7.9250        S  Third   
3         1       1  female  35.0      1      0  53.1000        S  First   
4         0       3    male  35.0      0      0   8.0500        S  Third   

     who  adult_male deck  embark_town alive  alone  
0    man        True  NaN  Southampton    no  False  
1  woman       False    C    Cherbourg   yes  False  
2  woman       False  NaN  Southampton   yes   True  
3  woman       False    C  Southampton   yes  False  
4    man        True  NaN  Southampton    no   True  </code></pre>
</div>
</div>
<section id="getting-ready" class="level4">
<h4 class="anchored" data-anchor-id="getting-ready">(1) Getting Ready</h4>
<p>Then, we choose the variables to use in the classification, drop or replace <code>NA</code> values, and further process the data to get things ready.</p>
<div id="c233f190" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df[[<span class="st">'survived'</span>, <span class="st">'pclass'</span>, <span class="st">'sex'</span>, <span class="st">'age'</span>, <span class="st">'sibsp'</span>, <span class="st">'parch'</span>, <span class="st">'embarked'</span>]]</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Dropping obs with NAs in age</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.dropna(subset <span class="op">=</span> [<span class="st">'age'</span>], axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Replacing obs with NAs in embarked with the mode value</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>temp <span class="op">=</span> df[<span class="st">'embarked'</span>].value_counts(dropna<span class="op">=</span> <span class="va">True</span>).idxmax()</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(temp)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>temp <span class="op">=</span> df[<span class="st">'embarked'</span>].fillna(temp) </span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'embarked'</span>] <span class="op">=</span> temp</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>S
     survived  pclass     sex   age  sibsp  parch embarked
0           0       3    male  22.0      1      0        S
1           1       1  female  38.0      1      0        C
2           1       3  female  26.0      0      0        S
3           1       1  female  35.0      1      0        S
4           0       3    male  35.0      0      0        S
..        ...     ...     ...   ...    ...    ...      ...
885         0       3  female  39.0      0      5        Q
886         0       2    male  27.0      0      0        S
887         1       1  female  19.0      0      0        S
889         1       1    male  26.0      0      0        C
890         0       3    male  32.0      0      0        Q

[714 rows x 7 columns]</code></pre>
</div>
</div>
<div id="f121fe74" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating dummies for sex and embarked (one-hot-encoding)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>onehot_sex <span class="op">=</span> pd.get_dummies(df[<span class="st">'sex'</span>])</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>onehot_embarked <span class="op">=</span> pd.get_dummies(df[<span class="st">'embarked'</span>], prefix <span class="op">=</span> <span class="st">'town'</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.concat([df, onehot_sex], axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.concat([df, onehot_embarked], axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.drop([<span class="st">'sex'</span>, <span class="st">'embarked'</span>], axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[[<span class="st">'pclass'</span>, <span class="st">'age'</span>, <span class="st">'sibsp'</span>, <span class="st">'parch'</span>, <span class="st">'female'</span>, <span class="st">'male'</span>, <span class="st">'town_C'</span>, <span class="st">'town_Q'</span>, <span class="st">'town_S'</span>]]</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[[<span class="st">'survived'</span>]]</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     survived  pclass   age  sibsp  parch  female   male  town_C  town_Q  \
0           0       3  22.0      1      0   False   True   False   False   
1           1       1  38.0      1      0    True  False    True   False   
2           1       3  26.0      0      0    True  False   False   False   
3           1       1  35.0      1      0    True  False   False   False   
4           0       3  35.0      0      0   False   True   False   False   
..        ...     ...   ...    ...    ...     ...    ...     ...     ...   
885         0       3  39.0      0      5    True  False   False    True   
886         0       2  27.0      0      0   False   True   False   False   
887         1       1  19.0      0      0    True  False   False   False   
889         1       1  26.0      0      0   False   True    True   False   
890         0       3  32.0      0      0   False   True   False    True   

     town_S  
0      True  
1     False  
2      True  
3      True  
4      True  
..      ...  
885   False  
886    True  
887    True  
889   False  
890   False  

[714 rows x 10 columns]</code></pre>
</div>
</div>
<p><br></p>
</section>
<section id="normalization-and-splitting-data" class="level4">
<h4 class="anchored" data-anchor-id="normalization-and-splitting-data">(2) Normalization and Splitting data</h4>
<p>One of the most important factors in distance-based algorithms(KNN, K-Means, etc.) and gradient-based algorithms(logistic regression, neural networks) is the <strong><em>normalization</em></strong> of predictor values. This is because input values with larger scales could have a larger effect on calculating the distance between observations. To eliminate such concern, normalization is necessary. In <code>sci-kit learn</code>, we do this with the <code>preprocessing</code> module.</p>
<div id="e8986259" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> preprocessing</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Before:'</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> preprocessing.StandardScaler().fit(X).transform(X)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'After:'</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pd.DataFrame(X))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Before:
     pclass   age  sibsp  parch  female   male  town_C  town_Q  town_S
0         3  22.0      1      0   False   True   False   False    True
1         1  38.0      1      0    True  False    True   False   False
2         3  26.0      0      0    True  False   False   False    True
3         1  35.0      1      0    True  False   False   False    True
4         3  35.0      0      0   False   True   False   False    True
..      ...   ...    ...    ...     ...    ...     ...     ...     ...
885       3  39.0      0      5    True  False   False    True   False
886       2  27.0      0      0   False   True   False   False    True
887       1  19.0      0      0    True  False   False   False    True
889       1  26.0      0      0   False   True    True   False   False
890       3  32.0      0      0   False   True   False    True   False

[714 rows x 9 columns]
After:
            0         1         2         3         4         5         6  \
0    0.911232 -0.530377  0.524570 -0.505895 -0.759051  0.759051 -0.471808   
1   -1.476364  0.571831  0.524570 -0.505895  1.317434 -1.317434  2.119506   
2    0.911232 -0.254825 -0.551703 -0.505895  1.317434 -1.317434 -0.471808   
3   -1.476364  0.365167  0.524570 -0.505895  1.317434 -1.317434 -0.471808   
4    0.911232  0.365167 -0.551703 -0.505895 -0.759051  0.759051 -0.471808   
..        ...       ...       ...       ...       ...       ...       ...   
709  0.911232  0.640719 -0.551703  5.357890  1.317434 -1.317434 -0.471808   
710 -0.282566 -0.185937 -0.551703 -0.505895 -0.759051  0.759051 -0.471808   
711 -1.476364 -0.737041 -0.551703 -0.505895  1.317434 -1.317434 -0.471808   
712 -1.476364 -0.254825 -0.551703 -0.505895 -0.759051  0.759051  2.119506   
713  0.911232  0.158503 -0.551703 -0.505895 -0.759051  0.759051 -0.471808   

            7         8  
0   -0.202031  0.533078  
1   -0.202031 -1.875896  
2   -0.202031  0.533078  
3   -0.202031  0.533078  
4   -0.202031  0.533078  
..        ...       ...  
709  4.949747 -1.875896  
710 -0.202031  0.533078  
711 -0.202031  0.533078  
712 -0.202031 -1.875896  
713  4.949747 -1.875896  

[714 rows x 9 columns]</code></pre>
</div>
</div>
<p>In the code above, <code>preprocessing.StandardScaler()</code> instantiates a scaling process, and the first method, <code>fit(X)</code>, receives <code>X</code> and returns the sample mean and the sample variance of each column. Then, the second method, <code>transform(X)</code> returns the rescaled (transformed) X values according to the values from <code>fit(X)</code>. We replace the original <code>X</code> with this new instantiation of the scaler.</p>
<p><br></p>
<p>Finally, the last stage of preprocessing data is to split the data into a training set and a test set. Here, we split them in a 7:3 ratio. Also, we often want the <code>y</code> values to occur symmetrically for the training and test sets. To do this, we add the optional argument <code>stratify=y</code>.</p>
<div id="f799af00" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size <span class="op">=</span> <span class="fl">0.3</span>, random_state <span class="op">=</span> <span class="dv">10</span>, stratify <span class="op">=</span> y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>
</section>
<section id="training-and-testing" class="level4">
<h4 class="anchored" data-anchor-id="training-and-testing">(3) Training and Testing</h4>
<div id="63828026" class="cell" data-message="false" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># instantiating KNN</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsClassifier(n_neighbors <span class="op">=</span> <span class="dv">6</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># training the data</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>knn.fit(X_train, y_train)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># predicting(classifying)</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>y_predict <span class="op">=</span> knn.predict(X_test)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y_predict[<span class="dv">0</span>:<span class="dv">10</span>])</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y_test.values[<span class="dv">0</span>:<span class="dv">10</span>].flatten())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1 0 0 1 1 1 0 1 0 1]
[1 0 0 0 1 0 0 1 0 0]</code></pre>
</div>
</div>
<p><br></p>
</section>
<section id="model-performance" class="level4">
<h4 class="anchored" data-anchor-id="model-performance">(4) Model Performance</h4>
<p>After train/test process is done, we summarize the model performance with confusion matrix and the classification report.</p>
<p>For detailed explanation of measuring model performances in supervised learning, go to <a href="#sec-model_performance" class="quarto-xref">Section&nbsp;5</a>.</p>
<div id="886ccf5f" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> metrics</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># confusion matrix</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>knn_matrix <span class="op">=</span> metrics.confusion_matrix(y_test, y_predict)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Confusion Matrix:'</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(knn_matrix)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co"># accuracy</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>knn_score <span class="op">=</span> knn.score(X_test, y_test)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">Accuracy:'</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(knn_score)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co"># classification report</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>class_report <span class="op">=</span> metrics.classification_report(y_test, y_predict)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">Classification Report:'</span>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_report)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix:
[[114  14]
 [ 28  59]]

Accuracy:
0.8046511627906977

Classification Report:
              precision    recall  f1-score   support

           0       0.80      0.89      0.84       128
           1       0.81      0.68      0.74        87

    accuracy                           0.80       215
   macro avg       0.81      0.78      0.79       215
weighted avg       0.81      0.80      0.80       215
</code></pre>
</div>
</div>
<p><br></p>
<p>To determine the best <code>k</code> value, we can loop through different values of <code>k</code> and plot the <strong>model complexity curve</strong>.</p>
<p>In <a href="#fig-complexity_curve" class="quarto-xref">Figure&nbsp;1</a>, we see that after k=15, the model suffers from underfitting. However, below k=5, we can also say that the model suffers from overfitting.</p>
<div id="cell-fig-complexity_curve" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>train_accuracies <span class="op">=</span> {}</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>test_accuracies <span class="op">=</span> {}</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>neighbors <span class="op">=</span> np.arange(<span class="dv">1</span>,<span class="dv">26</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> neighbors:</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>  knn <span class="op">=</span> KNeighborsClassifier(n_neighbors <span class="op">=</span> k)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>  knn.fit(X_train, y_train)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>  train_accuracies[k] <span class="op">=</span> knn.score(X_train, y_train)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>  test_accuracies[k] <span class="op">=</span> knn.score(X_test, y_test)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize <span class="op">=</span> (<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'KNN Accuracy Plot'</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>plt.plot(neighbors, train_accuracies.values(), label <span class="op">=</span> <span class="st">'Training Accuracy'</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>plt.plot(neighbors, test_accuracies.values(), label <span class="op">=</span> <span class="st">'Test Accuracy'</span>)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of Neighbors"</span>)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Accuracy"</span>)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-complexity_curve" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-complexity_curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="ML_Python_files/figure-html/fig-complexity_curve-output-1.png" width="597" height="449" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-complexity_curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Model Complexity Curve
</figcaption>
</figure>
</div>
</div>
</div>
<p><br></p>
</section>
</section>
<section id="sec-tree" class="level3">
<h3 class="anchored" data-anchor-id="sec-tree">Tree-based Models(Decision Tree and Random Forests)</h3>
<section id="decision-trees" class="level4">
<h4 class="anchored" data-anchor-id="decision-trees">Decision Trees</h4>
<p>Trees are methods applicable to both classification and prediction problems. In such sense, they’re often called CART(Classification and Regression Tree). Here, we focus on decision trees, which classify the labels of data. Decision trees are based on a mechanism often called <strong>recursive partitioning</strong>. For details of recursive partitioning, please check <a href="./#sec-rpart">A.2.</a></p>
<p>Let’s begin with the following example workflow. In this workflow, we will use the <code>titanic</code> data from the <code>seaborn</code> package and predict whether a person survived or not with the predictor variables <code>pclass</code>, <code>sex</code>, <code>age</code>, <code>sibsp</code>, <code>parch</code>, and <code>embarked</code>. First, load the required packages and the dataset. The module for decision trees in <code>scikit-learn</code> is the <code>DecisionTreeClassifier</code> module in the <code>sklearn.tree</code> package.</p>
</section>
<section id="getting-ready-1" class="level4">
<h4 class="anchored" data-anchor-id="getting-ready-1">(1) Getting Ready</h4>
<div id="aa3fb6f3" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> sns.load_dataset(<span class="st">'titanic'</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df[[<span class="st">'survived'</span>, <span class="st">'pclass'</span>, <span class="st">'sex'</span>, <span class="st">'age'</span>, <span class="st">'sibsp'</span>, <span class="st">'parch'</span>, <span class="st">'embarked'</span>]]</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Dropping obs with NAs in age</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.dropna(subset <span class="op">=</span> [<span class="st">'age'</span>], axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Replacing obs with NAs in embarked with the mode value</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>temp <span class="op">=</span> df[<span class="st">'embarked'</span>].value_counts(dropna<span class="op">=</span> <span class="va">True</span>).idxmax()</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(temp)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>temp <span class="op">=</span> df[<span class="st">'embarked'</span>].fillna(temp) </span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'embarked'</span>] <span class="op">=</span> temp</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>S
     survived  pclass     sex   age  sibsp  parch embarked
0           0       3    male  22.0      1      0        S
1           1       1  female  38.0      1      0        C
2           1       3  female  26.0      0      0        S
3           1       1  female  35.0      1      0        S
4           0       3    male  35.0      0      0        S
..        ...     ...     ...   ...    ...    ...      ...
885         0       3  female  39.0      0      5        Q
886         0       2    male  27.0      0      0        S
887         1       1  female  19.0      0      0        S
889         1       1    male  26.0      0      0        C
890         0       3    male  32.0      0      0        Q

[714 rows x 7 columns]</code></pre>
</div>
</div>
<p>In contrast to <code>KNeighborsClassifier</code>, <code>DecisionTreeClassifier</code> does not require one-hot encoding and standardizing. However, it does require each string variable to be converted to integers using <code>label encoding</code>.</p>
<div id="f9f4f4f4" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Automatically encode all object (categorical) columns</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>categorical_columns <span class="op">=</span> df.select_dtypes(include<span class="op">=</span>[<span class="st">'object'</span>]).columns</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>label_encoders <span class="op">=</span> {}</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> column <span class="kw">in</span> categorical_columns:</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    le <span class="op">=</span> LabelEncoder()</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    df[column] <span class="op">=</span> le.fit_transform(df[column])</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    label_encoders[column] <span class="op">=</span> le</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co"># You can check, for example, how the variable sex is encoded with the .classes_ method of label_encoders['sex'] object</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(label_encoders[<span class="st">'sex'</span>].classes_) <span class="co"># female: 0, male: 1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     survived  pclass  sex   age  sibsp  parch  embarked
0           0       3    1  22.0      1      0         2
1           1       1    0  38.0      1      0         0
2           1       3    0  26.0      0      0         2
3           1       1    0  35.0      1      0         2
4           0       3    1  35.0      0      0         2
..        ...     ...  ...   ...    ...    ...       ...
885         0       3    0  39.0      0      5         1
886         0       2    1  27.0      0      0         2
887         1       1    0  19.0      0      0         2
889         1       1    1  26.0      0      0         0
890         0       3    1  32.0      0      0         1

[714 rows x 7 columns]
['female' 'male']</code></pre>
</div>
</div>
<p><br></p>
</section>
<section id="training-and-testing-1" class="level4">
<h4 class="anchored" data-anchor-id="training-and-testing-1">(2) Training and Testing</h4>
<p>Next, we split the data into a training set and a test set. After that we instantiate a <code>DecisionTreeClassifier</code> object, setting the maximum depth of a tree, <code>max_depth</code> to 2. <code>random_state</code> controls - sets the seed - any randomness that could happen in the decision-making process. Such randomness can happen when handling ties of multiple features, or when feature selection or bootstrap sampling is employed(We’ll discuss this later).</p>
<div id="e47003a7" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> plot_tree</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[[<span class="st">'pclass'</span>, <span class="st">'age'</span>, <span class="st">'sibsp'</span>, <span class="st">'parch'</span>, <span class="st">'sex'</span>, <span class="st">'embarked'</span>]]</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[[<span class="st">'survived'</span>]]</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size <span class="op">=</span> <span class="fl">0.3</span>, random_state <span class="op">=</span> <span class="dv">10</span>, stratify <span class="op">=</span> y)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate decision_tree</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>decision_tree <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit decision_tree to the training data</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>decision_tree.fit(X_train, y_train)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the tree</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>plot_tree(decision_tree.fit(X_train, y_train))</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="ML_Python_files/figure-html/cell-15-output-1.png" width="540" height="389" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>TBU</p>
</section>
</section>
<section id="logistic-regressions" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regressions">Logistic Regressions</h3>
<p>Logistic regressions are useful for classifying binary labels. We will do this with the same dataset, <code>titanic</code>, and classify whether passengers survived or not. The following code repeats the data preprocessing, normalization, and splitting process we had in the former examples.</p>
<div id="14c219ff" class="cell" data-message="false" data-execution_count="15">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> preprocessing</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> sns.load_dataset(<span class="st">'titanic'</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df[[<span class="st">'survived'</span>, <span class="st">'pclass'</span>, <span class="st">'sex'</span>, <span class="st">'age'</span>, <span class="st">'sibsp'</span>, <span class="st">'parch'</span>, <span class="st">'embarked'</span>]]</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Dropping obs with NAs in age</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.dropna(subset <span class="op">=</span> [<span class="st">'age'</span>], axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Replacing obs with NAs in embarked with the mode value</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>temp <span class="op">=</span> df[<span class="st">'embarked'</span>].value_counts(dropna<span class="op">=</span> <span class="va">True</span>).idxmax()</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>temp <span class="op">=</span> df[<span class="st">'embarked'</span>].fillna(temp) </span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'embarked'</span>] <span class="op">=</span> temp</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>onehot_sex <span class="op">=</span> pd.get_dummies(df[<span class="st">'sex'</span>])</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>onehot_embarked <span class="op">=</span> pd.get_dummies(df[<span class="st">'embarked'</span>], prefix <span class="op">=</span> <span class="st">'town'</span>)</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.concat([df, onehot_sex], axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.concat([df, onehot_embarked], axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.drop([<span class="st">'sex'</span>, <span class="st">'embarked'</span>], axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[[<span class="st">'pclass'</span>, <span class="st">'age'</span>, <span class="st">'sibsp'</span>, <span class="st">'parch'</span>, <span class="st">'female'</span>, <span class="st">'male'</span>, <span class="st">'town_C'</span>, <span class="st">'town_Q'</span>, <span class="st">'town_S'</span>]]</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[[<span class="st">'survived'</span>]]</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> preprocessing.StandardScaler().fit(X).transform(X)</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size <span class="op">=</span> <span class="fl">0.3</span>, random_state <span class="op">=</span> <span class="dv">10</span>, stratify <span class="op">=</span> y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="training-and-testing-2" class="level4">
<h4 class="anchored" data-anchor-id="training-and-testing-2">(1) Training and Testing</h4>
<p>Next, we instantiate a <code>LogisticRegression()</code> object and train it with the data. The <code>.predict(X_test)</code> method returns the predicted class of the test data, and <code>.predict_proba(X_test)</code> returns the predicted probability of each class.</p>
<ul>
<li><code>.predict(X_test)</code>: predicted class</li>
<li><code>.predict_proba(X_test)</code> : predicted probability of each class(the probability threshold is 0.5)</li>
<li><code>.predict_log_proba(X_test)</code> : predicted log-probability of each class</li>
</ul>
<div id="8e75937a" class="cell" data-message="false" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>logreg <span class="op">=</span> LogisticRegression()</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>logreg.fit(X_train, y_train)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> logreg.predict(X_test)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>y_pred_probs <span class="op">=</span> logreg.predict_proba(X_test)[:, <span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="roc--auc" class="level4">
<h4 class="anchored" data-anchor-id="roc--auc">(2) ROC- AUC</h4>
<p>By default, probability threshold for logistic regressions in <code>scikit-learn</code> is 0.5. However, the right threshold could vary across cases. Thus, to obtain the right threshold, we could plot <span class="math inline">\(1-\mathrm{specificity}\)</span> (on the X-axis) and <span class="math inline">\(\mathrm{sensitivity}\)</span> (on the Y-axis), which is called the <strong>ROC curve(Receiver Operating Characteristics)</strong>.</p>
<div id="be92ece2" class="cell" data-message="false" data-execution_count="17">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>one_m_spec, sens, thresholds <span class="op">=</span> roc_curve(y_test, y_pred_probs)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>,<span class="dv">1</span>], [<span class="dv">0</span>,<span class="dv">1</span>], <span class="st">'k--'</span>)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>plt.plot(one_m_spec, sens)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"False Positive Rate(1-Spec)"</span>)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"True Positive Rate(Sens)"</span>)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Logistic Regression ROC Curve"</span>)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="ML_Python_files/figure-html/cell-18-output-1.png" width="589" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>ROC curve illustrates how the model performs on the test data. The more the shape of ROC curve looks like <span class="math inline">\(\Gamma\)</span>, the better. - we can find a threshold that attains larger sensitivity and specificity - Area under the curve, <strong>AUC</strong>, is the summarized value of such model performance. Its value range between 0 and 1, with 1 being ideal.</p>
<div id="d4e6e64d" class="cell" data-message="false" data-execution_count="18">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_auc_score</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(roc_auc_score(y_test, y_pred_probs))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.836206896551724</code></pre>
</div>
</div>
<p><br></p>
</section>
</section>
</section>
<section id="predictions" class="level2">
<h2 class="anchored" data-anchor-id="predictions">Predictions</h2>
<p>Prediction is simply running regressions and using the fitted values as the predicted values of outcome. The process of such practice is not much different from classifications. Let’s begin by loading the data.</p>
<div id="1dacb59d" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> sns.load_dataset(<span class="st">'penguins'</span>)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="134">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">species</th>
<th data-quarto-table-cell-role="th">island</th>
<th data-quarto-table-cell-role="th">bill_length_mm</th>
<th data-quarto-table-cell-role="th">bill_depth_mm</th>
<th data-quarto-table-cell-role="th">flipper_length_mm</th>
<th data-quarto-table-cell-role="th">body_mass_g</th>
<th data-quarto-table-cell-role="th">sex</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Adelie</td>
<td>Torgersen</td>
<td>39.1</td>
<td>18.7</td>
<td>181.0</td>
<td>3750.0</td>
<td>Male</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Adelie</td>
<td>Torgersen</td>
<td>39.5</td>
<td>17.4</td>
<td>186.0</td>
<td>3800.0</td>
<td>Female</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Adelie</td>
<td>Torgersen</td>
<td>40.3</td>
<td>18.0</td>
<td>195.0</td>
<td>3250.0</td>
<td>Female</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>Adelie</td>
<td>Torgersen</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Adelie</td>
<td>Torgersen</td>
<td>36.7</td>
<td>19.3</td>
<td>193.0</td>
<td>3450.0</td>
<td>Female</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>With the <code>penguins</code> data from the <code>seaborn</code> package, we’d like to predict each penguin’s <code>body_mass_g</code> based on its <code>species</code>, <code>bill_length_mm</code>, <code>bill_depth_mm</code>, <code>flipper_length_mm</code>, and <code>sex</code>.</p>
<div id="b2272239" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.dropna(subset <span class="op">=</span> [<span class="st">'sex'</span>, <span class="st">'body_mass_g'</span>], how <span class="op">=</span> <span class="st">'any'</span>, axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>onehot_sex <span class="op">=</span> pd.get_dummies(df[<span class="st">'sex'</span>])</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>onehot_species <span class="op">=</span> pd.get_dummies(df[<span class="st">'species'</span>], prefix <span class="op">=</span> <span class="st">'species'</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.concat([df, onehot_sex], axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.concat([df, onehot_species], axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.drop([<span class="st">'sex'</span>, <span class="st">'species'</span>], axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(<span class="st">'body_mass_g'</span>, axis <span class="op">=</span> <span class="dv">1</span>).values</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[[<span class="st">'body_mass_g'</span>]]</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        island  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \
0    Torgersen            39.1           18.7              181.0       3750.0   
1    Torgersen            39.5           17.4              186.0       3800.0   
2    Torgersen            40.3           18.0              195.0       3250.0   
4    Torgersen            36.7           19.3              193.0       3450.0   
5    Torgersen            39.3           20.6              190.0       3650.0   
..         ...             ...            ...                ...          ...   
338     Biscoe            47.2           13.7              214.0       4925.0   
340     Biscoe            46.8           14.3              215.0       4850.0   
341     Biscoe            50.4           15.7              222.0       5750.0   
342     Biscoe            45.2           14.8              212.0       5200.0   
343     Biscoe            49.9           16.1              213.0       5400.0   

     Female   Male  species_Adelie  species_Chinstrap  species_Gentoo  
0     False   True            True              False           False  
1      True  False            True              False           False  
2      True  False            True              False           False  
4      True  False            True              False           False  
5     False   True            True              False           False  
..      ...    ...             ...                ...             ...  
338    True  False           False              False            True  
340    True  False           False              False            True  
341   False   True           False              False            True  
342    True  False           False              False            True  
343   False   True           False              False            True  

[333 rows x 10 columns]</code></pre>
</div>
</div>
<section id="sec-univ_reg" class="level3">
<h3 class="anchored" data-anchor-id="sec-univ_reg">Univariate Regression</h3>
<p>A univariate regression, or more specifically a simple linear regression, regresses <code>y</code> on a single predictor variable <code>x</code>. Let’s consider a simple case of regressing <code>body_mass_g</code> on <code>flipper_length_mm</code>.</p>
<div id="c143a32d" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize <span class="op">=</span> (<span class="dv">9</span>,<span class="dv">5</span>))</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>ax1 <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>ax2 <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>df.plot(kind<span class="op">=</span><span class="st">'scatter'</span>, x<span class="op">=</span><span class="st">'flipper_length_mm'</span>, y<span class="op">=</span><span class="st">'body_mass_g'</span>, c<span class="op">=</span><span class="st">'coral'</span>, ax<span class="op">=</span>ax1)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">'flipper_length_mm'</span>, y<span class="op">=</span><span class="st">'body_mass_g'</span>, data<span class="op">=</span>df, scatter_kws<span class="op">=</span>{<span class="st">"alpha"</span>: <span class="fl">0.5</span>}, ax<span class="op">=</span>ax2)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="ML_Python_files/figure-html/cell-22-output-1.png" width="750" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In the code below, we will split the data into training set and test set(7:3), calculate the model performance with R-squared in the test set, and print out the linear coefficients.</p>
<div id="ce1ad511" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> df[[<span class="st">'flipper_length_mm'</span>]]</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[[<span class="st">'body_mass_g'</span>]]</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co"># split the data</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>x_train, x_test, y_train, y_test <span class="op">=</span> train_test_split(x, y, test_size <span class="op">=</span> <span class="fl">0.3</span>, random_state <span class="op">=</span> <span class="dv">10</span>)</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="co"># instantiate linear regression</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>reg <span class="op">=</span> LinearRegression()</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="co"># train the data</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>reg.fit(x_train, y_train)</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate R-squared</span></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>r_square <span class="op">=</span> reg.score(x_test, y_test)</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'R-squared: '</span>, r_square)</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a><span class="co"># print the slope and the intercept</span></span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'slope: '</span>, reg.coef_)</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'intercept: '</span>, reg.intercept_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>R-squared:  0.6908213440414697
slope:  [[50.50455071]]
intercept:  [-5947.85540739]</code></pre>
</div>
</div>
</section>
<section id="multinomial-regression-with-polynomialfeatures" class="level3">
<h3 class="anchored" data-anchor-id="multinomial-regression-with-polynomialfeatures">Multinomial Regression with <code>PolynomialFeatures</code></h3>
<p>Following the codes in <a href="#sec-univ_reg" class="quarto-xref">Section&nbsp;4.1</a>, we will introduce a quadratic and a cubic term of X to the regression.</p>
<div id="ed8c6255" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> df[[<span class="st">'flipper_length_mm'</span>]]</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[[<span class="st">'body_mass_g'</span>]]</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="co"># split the data</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>x_train, x_test, y_train, y_test <span class="op">=</span> train_test_split(x, y, test_size <span class="op">=</span> <span class="fl">0.3</span>, random_state <span class="op">=</span> <span class="dv">10</span>)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="co"># adding polynomials</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>poly <span class="op">=</span> PolynomialFeatures(degree <span class="op">=</span> <span class="dv">3</span>)</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>x_train_poly <span class="op">=</span> poly.fit_transform(x_train)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>x_test_poly <span class="op">=</span> poly.fit_transform(x_test)</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'x_train shape: '</span>, x_train.shape)</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'x_train_poly shape: '</span>, x_train_poly.shape)</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="co"># instantiate linear regression</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>reg <span class="op">=</span> LinearRegression()</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a><span class="co"># train the data</span></span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>reg.fit(x_train_poly, y_train)</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate R-squared</span></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>r_square <span class="op">=</span> reg.score(x_test_poly, y_test)</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'R-squared: '</span>, r_square)</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a><span class="co"># print the slope and the intercept</span></span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'slope: '</span>, reg.coef_)</span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'intercept: '</span>, reg.intercept_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x_train shape:  (233, 1)
x_train_poly shape:  (233, 4)
R-squared:  0.7277186999187835
slope:  [[ 0.00000000e+00 -2.19281026e+03  1.06376941e+01 -1.67427890e-02]]
intercept:  [151056.40370382]</code></pre>
</div>
</div>
<p><br></p>
</section>
<section id="multivariate-regressions" class="level3">
<h3 class="anchored" data-anchor-id="multivariate-regressions">Multivariate Regressions</h3>
<div id="2852f19a" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop([<span class="st">'body_mass_g'</span>, <span class="st">'island'</span>, <span class="st">'Male'</span>, <span class="st">'species_Gentoo'</span>], axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[[<span class="st">'body_mass_g'</span>]]</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co"># split the data</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size <span class="op">=</span> <span class="fl">0.3</span>, random_state <span class="op">=</span> <span class="dv">10</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co"># instantiate linear regression</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>reg <span class="op">=</span> LinearRegression()</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co"># train the data</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>reg.fit(X_train, y_train)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate R-squared</span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>r_square <span class="op">=</span> reg.score(X_test, y_test)</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'R-squared: '</span>, r_square)</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="co"># print the slope and the intercept</span></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'covariates: '</span>, <span class="bu">list</span>(X_test.columns))</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'slope: '</span>, reg.coef_)</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'intercept: '</span>, reg.intercept_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>R-squared:  0.8480141789076938
covariates:  ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'Female', 'species_Adelie', 'species_Chinstrap']
slope:  [[   12.26524581    77.29979709    18.82252667  -366.33908719
  -1022.18520551 -1190.49839881]]
intercept:  [-574.49999499]</code></pre>
</div>
</div>
<p><br></p>
</section>
<section id="penalized-regressions" class="level3">
<h3 class="anchored" data-anchor-id="penalized-regressions">Penalized Regressions</h3>
<p>Penalized regressions, or regularized regressions, are regressions with loss functions modified to reduce the possibility of overfitting. Usually, large coefficients lead to overfitting. Thus, penalized regressions such as ridge regressions and LASSO(Least Absolute Shrinkage and Selection Operator) penalizes large sized coefficients, by including the size of coefficients into the loss function.</p>
<ul>
<li>Note that since the size of coefficients could differ across measure units of variables, <strong><em>standardization before regression is indispensable!!!</em></strong></li>
<li>Also, the coefficient for the intercept(<span class="math inline">\(\beta_0\)</span>) is not included into the penalty term.</li>
</ul>
<p><br></p>
<section id="ridge-regressions" class="level4">
<h4 class="anchored" data-anchor-id="ridge-regressions">(1) Ridge Regressions</h4>
<ul>
<li><strong>Loss Function</strong> = <span class="math inline">\(\mathrm{MSE} + \alpha\times\sum_{i=1}^k\beta_i^2\)</span></li>
<li>Here, <span class="math inline">\(\alpha\)</span> is called a <em>hyperparameter</em>, a parameter that we choose to optimize the prediction.</li>
<li>In the case of <span class="math inline">\(\alpha = 0\)</span>, ridge regrerssions would be equivalent to OLS.</li>
</ul>
<div id="87756691" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="co"># scores = [] #initiate an empty list to record R-squared</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co"># for alpha in range(-1, 4): </span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co">#   ridge = Ridge(alpha = 10**alpha)</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="co">#   ridge.fit(X_train, y_train)</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="co">#   y_pred = ridge.predict(X_test)</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="co">#   scores.append(ridge.score(X_test, y_test))</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a><span class="co"># print(scores)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="lassos" class="level4">
<h4 class="anchored" data-anchor-id="lassos">(2) LASSOs</h4>
<ul>
<li><strong>Loss Function</strong> = <span class="math inline">\(\mathrm{MSE} + \alpha\times\sum_{i=1}^k|\beta_i|\)</span></li>
<li>Here, <span class="math inline">\(\alpha\)</span> is called a <em>hyperparameter</em>, a parameter that we choose to optimize the prediction.</li>
<li>In the case of <span class="math inline">\(\alpha = 0\)</span>, LASSO would be equivalent to OLS.</li>
<li>LASSO shrinks the coefficients of less important features to zero</li>
</ul>
<div id="e342b00c" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># from sklearn.linear_model import Lasso</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="co"># lasso = Lasso(alpha=0.1)</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="co"># lasso_coef = lasso.fit(X, y).coef_</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
</section>
<section id="sec-model_performance" class="level2">
<h2 class="anchored" data-anchor-id="sec-model_performance">Methods of Measuring Model Performance</h2>
<p>Model performance is primarily assessed with the test set.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Objective</th>
<th style="text-align: left;">Classification</th>
<th style="text-align: left;">Prediction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Related Indices</td>
<td style="text-align: left;">- Accuracy<br> - Sensitivity(TP)<br> - Specificity(TN)</td>
<td style="text-align: left;">- R-squared<br> - MSE</td>
</tr>
<tr class="even">
<td style="text-align: left;">Related Figures/Tables</td>
<td style="text-align: left;">Confusion Matrix</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<section id="confusion-matrix" class="level3">
<h3 class="anchored" data-anchor-id="confusion-matrix">Confusion Matrix</h3>
<table>
<tbody><tr>
<th>
</th>
<th>
</th>
<th>
</th>
<th style="text-align: center;" colspan="2">
Actual Value
</th>
</tr>
<tr>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="text-align: center;">
T
</td>
<td style="text-align: center;">
F
</td>
</tr>
<tr>
<td style="text-align: center; padding-right: 30px;" rowspan="2">
<b>Predicted Value </b>
</td>
<td>
</td>
<td style="text-align: center;">
T
</td>
<td style="text-align: center;">
TP
</td>
<td style="text-align: center;">
FP
</td>
</tr>
<tr>
<td>
</td>
<td style="text-align: center;">
F
</td>
<td style="text-align: center;">
FN
</td>
<td style="text-align: center;">
TN
</td>
</tr>
</tbody></table>
<p><br></p>
</section>
<section id="accuracy" class="level3">
<h3 class="anchored" data-anchor-id="accuracy">Accuracy</h3>
<p><span class="math display">\[ \frac{\text{correct predictions}}{\text{total observations}} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} +  \text{FN}}\]</span></p>
<p><br></p>
</section>
<section id="sensitivity-and-specificity" class="level3">
<h3 class="anchored" data-anchor-id="sensitivity-and-specificity">Sensitivity and Specificity</h3>
<p><span class="math display">\[ \mathrm{Sensitivity(=Recall)} = \frac{\text{TP}}{\text{TP} +\text{FN}}\]</span></p>
<p><span class="math display">\[ \mathrm{Specificity} = \frac{\text{TN}}{\text{TN} +\text{FP}}\]</span></p>
<p><br></p>
</section>
<section id="precision-and-negative-predictive" class="level3">
<h3 class="anchored" data-anchor-id="precision-and-negative-predictive">Precision and Negative Predictive</h3>
<p><span class="math display">\[ \mathrm{Precision} = \frac{\text{TP}}{\text{TP} +\text{FP}}\]</span> <span class="math display">\[ \mathrm{NPV} = \frac{\text{TN}}{\text{TN} +\text{FN}}\]</span></p>
<p><br></p>
</section>
<section id="f1-score" class="level3">
<h3 class="anchored" data-anchor-id="f1-score">F1 Score</h3>
<p><span class="math display">\[\mathrm{F1} = 2\cdot\frac{\mathrm{Precision}\cdot\mathrm{Recall}}{\mathrm{Precision} + \mathrm{Recall}}\]</span></p>
</section>
<section id="r-squared-and-mse" class="level3">
<h3 class="anchored" data-anchor-id="r-squared-and-mse">R-Squared and MSE</h3>
<p>R-squared and mean squared error(MSE) are often reported as parts of linear regressions results. To understand the concept, we must understand the concept of <strong>loss function</strong> in regression.</p>
<p>Loss function represents the amount of information loss incurred by using the predicted values rather than actual target values. The most common loss function is the mean squared error function(a.k.a. quadratic loss function), which is equal to the residual sum of squares divided by the number of observations.</p>
<p><span class="math display">\[ MSE = \frac{1}{n}\sum_{i=1}^n(y_i-\hat{y}_i)^2 \]</span> <span class="math display">\[ RSS = \sum_{i=1}^n(y_i-\hat{y}_i)^2 \]</span></p>
<p><code>mean_squared_error</code> function from <code>sklearn.metrics</code> calculates the MSE of a given model.</p>
<div id="12bd0119" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>mean_squared_error(y_test, y_pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>R-squared(<span class="math inline">\(R^2\)</span>) quantifies the portion explained by the predictor variables of the variance of the target variable. The value ranges from 0 to 1, and a higher value implies that the predictor variables explain the target well.</p>
<p><span class="math display">\[ R^2 = 1-\frac{RSS}{TSS}\]</span> <span class="math display">\[ TSS = \sum_{i=1}^n(y_i-\bar{y})^2\]</span></p>
<p>It is calculated by the <code>score</code> method of a regression object.</p>
<div id="8e8b6601" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>r_square <span class="op">=</span> reg.score(X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="cross-validation-and-hyperparameter-tuning" class="level2">
<h2 class="anchored" data-anchor-id="cross-validation-and-hyperparameter-tuning">Cross Validation and Hyperparameter Tuning</h2>
<p>Cross validating means dividing the sample into <span class="math inline">\(k\)</span> folds, and rotate the role of test data. In each of these splits and the corresponding test-fold, we obtain <span class="math inline">\(k\)</span> model performance metrics. Through cross validation, we can check whether these <span class="math inline">\(k\)</span> metrics are similar or having peculiarities, helping us generalize the model performance to unseen data.</p>
<div id="be7bf0d9" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score, KFold</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> KFold(n_splits <span class="op">=</span> <span class="dv">5</span>, shuffle <span class="op">=</span> <span class="va">True</span>, random_state <span class="op">=</span> <span class="dv">42</span>) <span class="co"># "shuffle = True" shuffles the data before splitting the data</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>reg <span class="op">=</span> LinearRegression() <span class="co"># instantiation of the model selected</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>cv_results <span class="op">=</span> cross_val_score(reg, X, y, cv <span class="op">=</span> kf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Most ML methods require the researcher to set parameters that shape the learning process, and these parameters are called hyperparameters. Examples include number of neighbors in KNN, alpha in ridge and LASSO, and probability threshold in logistic regressions. Some hyperparameters display stronger predictive power than others in the test data. However, choosing hyperparameter based on the result from a single test set leads to an overfitting to the test set. <em>Thus, it is essential to use cross-validation for hyperparameter tuning.</em></p>
<p><br></p>
<section id="why-cross-validation-for-hyperparameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="why-cross-validation-for-hyperparameter-tuning">Why cross-validation for hyperparameter tuning?</h3>
<p>We have two kinds of parameters: <strong>(1) model parameters</strong>, which we estimate with the train data, and <strong>(2) hyperparameters</strong> which shapes the model.</p>
<p>Model parameters are estimated based on the training set, given a specific value of hyperparameters. Thus, we can’t obtain model parameter estimates and tune hyperparameters simultaneously in the training set, because the model parameter estimates are dependent on the hyperparameter value.</p>
<p>For example, in LASSO,</p>
<p><span id="eq-lasso_beta"><span class="math display">\[
\hat{\mathbf{\beta}} = \mathrm{arg}\min_\beta MSE(\beta; \xi^{tr})+\alpha\cdot||\beta||
\tag{1}\]</span></span></p>
<p>Here <span class="math inline">\(\xi^{tr}\)</span> denotes the training data. <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> denote the hyperparameter and the regression coefficients, respectively. Inspecting the definition, one can say <span class="math inline">\(\hat{\beta} = \hat{\beta}(\alpha, \xi^{tr})\)</span>.</p>
<p>If <span class="math inline">\(\alpha\)</span> is also determined by the training set, <span class="math inline">\(\hat{\beta}\)</span> reduces into <span class="math inline">\(\hat{\beta}(\alpha(\xi^{tr}), \xi^{tr}) = \hat{\beta}(\xi^{tr})\)</span>. (i.e., The parameters are entirely a function of the training set). Note that in the context of LASSO, this results the same coefficients to OLS as <span class="math inline">\(\alpha\)</span> would reduce to 0.</p>
<p>Then, should we select hyperparameters based on test set(i.e., given a set of possible candidates, should we select one that best performs in the test set)? Such hyperparameters would overfit the model to the test set; we want our hyperparameters to work well with unseen and new data.</p>
<p>In the former example with LASSO,</p>
<p><span class="math display">\[
\hat{\mathbf{\beta}_\alpha}(\xi^{tr}) = \mathrm{arg}\min_\beta MSE(\beta; \xi^{tr})+\alpha\cdot||\beta||
\]</span></p>
<p>is a function of <span class="math inline">\(\xi^{tr}\)</span> and <span class="math inline">\(\alpha\)</span>. In contrast to <a href="#eq-lasso_beta" class="quarto-xref">Equation&nbsp;1</a>, I added the subscript <span class="math inline">\(\alpha\)</span>, because this is not our final choice of <span class="math inline">\(\hat{\beta}\)</span>; rather, it is the interim value of <span class="math inline">\(\hat{\beta}\)</span> given a specific value of <span class="math inline">\(\alpha\)</span>. The final choice of <span class="math inline">\(\hat{\beta}\)</span> would be among <span class="math inline">\(\{\hat{\beta_\alpha}: \alpha \in I_\alpha\}\)</span>, whee <span class="math inline">\(I_\alpha\)</span> denotes the set of all <span class="math inline">\(\alpha\)</span>’s in consideration.</p>
<p>Let <span class="math inline">\(\xi^{te}\)</span> denote the test set. Comparing all the <span class="math display">\[
\hat{\alpha} = \mathrm{arg}\min_{\alpha\in I_\alpha} MSE(\hat{\beta_\alpha}(\xi^{tr}); \xi^{te})+\alpha\cdot||\hat{\beta_\alpha}||
\]</span></p>
<p>The resulting best <span class="math inline">\(\hat{\alpha}\)</span> is the function of <span class="math inline">\(\xi^{tr}, \xi^{te}\)</span>, and <span class="math inline">\(I_\alpha\)</span>, and our choice of <span class="math inline">\(\hat{\beta}\)</span> would be <span class="math inline">\(\hat{\beta_\hat{\alpha}}\)</span>.</p>
<p>This leads to overfitting the model to the test set, because the test set determines both <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span>.</p>
<p>Now, consider k-fold cross validation. All splits assign the role of a test set to different folds. Let <span class="math inline">\(S_i(i=1,2,...,k)\)</span> denote the split where the <span class="math inline">\(i\)</span>-th fold takes the role of a test set. Let <span class="math inline">\(\xi^i\)</span> and <span class="math inline">\(\xi^{-i}\)</span> denote the test fold and training folds of the split, <span class="math inline">\(S_i\)</span>. Just for clarity, I want to add one more notation, <span class="math inline">\(\Xi(k)\)</span>, which denotes the splitting scheme. Note that although the number <span class="math inline">\(k\)</span> is determined by the researcher, the splitting scheme <span class="math inline">\(\Xi(k)\)</span> is determined randomly.</p>
<p>Suppose now, we want to minimize the MSE from all the <span class="math inline">\(k\)</span> folds with respect to <span class="math inline">\(\alpha\)</span>.</p>
<p><span class="math display">\[
\hat{\alpha} = \mathrm{arg}\min_{\alpha\in I_\alpha} \sum_{i=1}^k\left(MSE(\hat{\beta_\alpha}(\xi^{-i}); \xi^{i})+\alpha\cdot||\hat{\beta_\alpha}||\right)
\]</span> Then, <span class="math inline">\(\hat{\alpha}\)</span> is a function of <span class="math inline">\(I_\alpha\)</span>, <span class="math inline">\(\Xi(k)\)</span> and <span class="math inline">\(k\)</span>. Now, although <span class="math inline">\(\hat{\alpha}\)</span> and the corresponding <span class="math inline">\(\hat{\beta}_\hat{\alpha}\)</span> are determined to fit the sample data well, they do not fit the model to a specific subset of the sample data. Instead, roughly speaking, they fit it to the splitting scheme <span class="math inline">\(\Xi(k)\)</span>, which is determined randomly. This is why cross validations are often necessary for comparing different models and hyperparameters.</p>
</section>
<section id="grid-search-cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="grid-search-cross-validation">Grid Search cross-validation</h3>
<p><code>GridSearchCV</code> method from the <code>sklearn.model_selection</code> module creates a grid of possible hyperparameter values. In detail, there could be more than one hyperparameters for a prediction model. - You can choose both the number of neighbors and a metric(Euclidean or L1) in the case of KNN. - <code>GridSearchCV</code> searches through all the user-suggested values of those user-chosen set of hyperparameters.</p>
<div id="d709d6c6" class="cell" data-message="false" data-execution_count="31">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold, GridSearchCV</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> KFold(n_splits <span class="op">=</span> <span class="dv">5</span>, shuffle <span class="op">=</span> <span class="va">True</span>, random_state <span class="op">=</span> <span class="dv">42</span>)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {<span class="st">'n_neighbors'</span>: <span class="bu">range</span>(<span class="dv">2</span>,<span class="dv">20</span>,<span class="dv">2</span>), <span class="st">'metric'</span>: [<span class="st">"minkowski"</span>, <span class="st">"cityblock"</span>]} <span class="co"># Note: Minkowski metric is virtually Euclidean in our case.</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsClassifier()</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>knn_cv <span class="op">=</span> GridSearchCV(knn, param_grid, cv <span class="op">=</span> kf)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>knn_cv.fit(X_train, y_train)</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(knn_cv.best_params_, knn_cv.best_score_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{'metric': 'minkowski', 'n_neighbors': 4} 0.7916363636363637</code></pre>
</div>
</div>
</section>
<section id="randomized-search-cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="randomized-search-cross-validation">Randomized Search cross-validation</h3>
<p><code>GridSearchCV</code> is indeed a very complete way for searching the best set of parameters, given a user-defined range to search for. However, this often results in unnecessarily many trials. That is, <code>GridSearchCV</code> goes through the combination of all the folds and the possible parameters values, and that is way too much sometimes. <code>RandomizedSearchCV</code> goes through a random set of parameter values, rather than exhausting all the candidates. <code>RandomizedSearchCV</code> requires one more argument than <code>GridSearchCV</code>: <code>n_iter</code>. <code>n_iter</code> is the number of possible hyperparameter sets to be tested. More <code>n_iter</code> leads to a finer quality at the cost of runtime.</p>
<div id="68289e0c" class="cell" data-message="false" data-execution_count="32">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold, RandomizedSearchCV</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> KFold(n_splits <span class="op">=</span> <span class="dv">5</span>, shuffle <span class="op">=</span> <span class="va">True</span>, random_state <span class="op">=</span> <span class="dv">42</span>)</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {<span class="st">'n_neighbors'</span>: <span class="bu">range</span>(<span class="dv">2</span>,<span class="dv">20</span>,<span class="dv">2</span>), <span class="st">'metric'</span>: [<span class="st">"minkowski"</span>, <span class="st">"cityblock"</span>]} </span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsClassifier()</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>knn_cv <span class="op">=</span> RandomizedSearchCV(knn, param_grid, cv <span class="op">=</span> kf, n_iter<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>knn_cv.fit(X_train, y_train)</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(knn_cv.best_params_, knn_cv.best_score_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{'n_neighbors': 4, 'metric': 'minkowski'} 0.7916363636363637</code></pre>
</div>
</div>
</section>
</section>
<section id="pipeline-operators" class="level2">
<h2 class="anchored" data-anchor-id="pipeline-operators">Pipeline Operators</h2>
<p>TBU</p>
</section>
<section id="scikit-learn-cheatsheets" class="level2">
<h2 class="anchored" data-anchor-id="scikit-learn-cheatsheets"><code>scikit-learn</code> Cheatsheets</h2>
<p>TBU</p>
<section id="handling-missing-data" class="level3">
<h3 class="anchored" data-anchor-id="handling-missing-data">Handling Missing Data</h3>
<div id="25850084" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.impute <span class="im">import</span> SimpleImputer</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co"># imputation with sample mean</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>imp_mean <span class="op">=</span> SimpleImputer()</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> imp_mode.transform(X_train)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="co"># imputation with mode</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>imp_mode <span class="op">=</span> SimpleImputer(strategy<span class="op">=</span><span class="st">"most_frequent"</span>)</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> imp_mode.transform(X_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<p>TBU</p>
<section id="sec-knn_algorithm" class="level3">
<h3 class="anchored" data-anchor-id="sec-knn_algorithm">A.1. KNN Algorithm</h3>
<p>TBU</p>
</section>
<section id="sec-rpart" class="level3">
<h3 class="anchored" data-anchor-id="sec-rpart">A.2. Recursive Partitioning</h3>
<p>In this section, I elaborate on how the tree-based models split the covariate space recursively. At each node(except for the terminal node), a tree first chooses one covariate and the cutoff that would most help the children nodes predict the outcome variable. This is the concept of recursive partitioning. At each node, the model recursively selects a covariate(i.e., feature) and a cutoff(i.e., split-point) to explain the outcome’s variance(or heterogeneity). However, the measure of how well the outcome heterogeneity is explained depends on whether the outcome is continuous or categorical. That is,</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Tree Type</th>
<th style="text-align: left;">Target Variable</th>
<th style="text-align: left;">Split Criterion</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Regression Trees</td>
<td style="text-align: left;">Continuous</td>
<td style="text-align: left;">minimize RSS, MSE</td>
</tr>
<tr class="even">
<td style="text-align: left;">Classification Trees</td>
<td style="text-align: left;">Categorical</td>
<td style="text-align: left;">maximize IG</td>
</tr>
</tbody>
</table>
<p>For regression trees, the aim of recursive partitioning is to choose a covariate and a split-point that would minimize the RSS(residual sum of squares). In other words, we minimize</p>
<p><span class="math display">\[
\sum_{i: x_i \in L(j,s)}(y_i-\overline{y}_L)^2 + \sum_{i: x_i \in R(j,s)}(y_i-\overline{y}_R)^2
\]</span></p>
<p>where</p>
<p><span class="math display">\[
L(j,s) = \{X|X_j &lt; s\} \\
R(j,s) = \{X|X_j \geq s\}
\]</span> and <span class="math inline">\(\overline{y}_L, \overline{y}_R\)</span> denote the mean outcome for the training observations in each region.</p>
<p>For classification trees, the partitioning aims to maximize the <strong>information gain(IG)</strong> from the split:</p>
<p><span class="math display">\[
IG(f, s) = Imp(parent) - \left(\frac{N_{L}}{N}Imp(L) +\frac{N_{R}}{N}Imp(R) \right)
\]</span> where</p>
<ul>
<li><span class="math inline">\(IG\)</span> : information gain</li>
<li><span class="math inline">\(Imp\)</span> : impurity index</li>
<li><span class="math inline">\(N, N_{L}, N_{R}\)</span> : Number of obs. in the parent ntode, left child node, right child node</li>
</ul>
<p>Here, <span class="math inline">\(Imp()\)</span> stands for the impurity of a specific node, and thus information gain represents the purity gained from splitting the node into two. Intuitively, we can think of the impurity as a measure of how mixed the labels(i.e., outcome variables) are at a certain node. Thus, a property that the impurity measure should have is to have a low value when the node fully consists of one label value, and have a high value if the node consists of multiple label values.</p>
<p>There are multiple ways to measure the impurity:</p>
<ul>
<li><strong>Gini Index</strong>: <span class="math inline">\(G(m) = \sum_{k=1}^K \overline{p}_{mk}(1-\overline{p}_{mk})\)</span></li>
<li><strong>Entropy</strong>: <span class="math inline">\(D(m)= -\sum_{k=1}^K\overline{p}_{mk}\log \overline{p}_{mk}\)</span></li>
</ul>
<p>where <span class="math inline">\(m\)</span> denotes the node.</p>
<p>At a glance, you can see that both gini and entropy goes to zero when each <span class="math inline">\(\overline{p}_{mk}\)</span> goes close to either zero or one. Note that <span class="math inline">\(\sum_{k=1}^K\overline{p}_{mk}=1\)</span>, and solving the “maximization” under linear constraints would demonstrate that <span class="math inline">\(\overline{p}_{mk}=\frac{1}{K}\)</span> mazimizes the impurity for both measures. These two measures are widely used in the context of ML, so it is good to know before we start making our trees and forests:)</p>
<section id="some-remarks-for-econ-students" class="level4">
<h4 class="anchored" data-anchor-id="some-remarks-for-econ-students">Some Remarks for Econ Students</h4>
<p>Notice that in the gini index, since <span class="math inline">\(\sum_{k=1}^K\overline{p}_{mk}=1\)</span>,</p>
<p><span class="math display">\[
G(m) = \sum_{k=1}^K \overline{p}_{mk}(1-\overline{p}_{mk})=1-\sum_{k=1}^K\overline{p}_{mk}^2
\]</span> which very much resembles the <strong>Herfindahl-Hirschman Index (HHI)</strong> in Industrial Organzation(an index for measuring the concentration of market share). Obviously, different disciplines sometimes rely on similar approaches.</p>
</section>
</section>
<section id="a.3.-lasso-vs-ols" class="level3">
<h3 class="anchored" data-anchor-id="a.3.-lasso-vs-ols">A.3. LASSO vs OLS</h3>
<p>LASSO shrinks the number of predictor variables used to explain the target. But after removing the bad predictors, <em>wouldn’t OLS with the remaining features be a better prediction than LASSO estimates?</em></p>
<p><br></p>
<p><em>The answer is</em> <strong><em>no.</em></strong> LASSO reduces the variance of the model at the cost of the model’s bias, whereas OLS, even with only the remaining features,increases variance and reduces bias.</p>
<div id="075c6476" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LassoCV, LinearRegression</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a><span class="co"># A simulation with higher dimensionality (p &gt; n) and multiple runs</span></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>n_samples, n_features <span class="op">=</span> <span class="dv">50</span>, <span class="dv">200</span>  <span class="co"># Higher dimensionality (p &gt; n)</span></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>n_simulations <span class="op">=</span> <span class="dv">100</span>  <span class="co"># Number of simulations</span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>mse_lasso_list <span class="op">=</span> []</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>mse_ols_list <span class="op">=</span> []</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_simulations):</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.random.randn(n_samples, n_features)</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>    true_coef <span class="op">=</span> np.zeros(n_features)</span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>    true_coef[:<span class="dv">10</span>] <span class="op">=</span> np.random.uniform(<span class="dv">5</span>, <span class="dv">10</span>, size<span class="op">=</span><span class="dv">10</span>)  <span class="co"># Sparse true coefficients</span></span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> X <span class="op">@</span> true_coef <span class="op">+</span> np.random.randn(n_samples) <span class="op">*</span> <span class="dv">5</span></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>    X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>    lasso <span class="op">=</span> LassoCV(cv<span class="op">=</span><span class="dv">5</span>).fit(X_train, y_train)</span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>    selected_features <span class="op">=</span> np.where(lasso.coef_ <span class="op">!=</span> <span class="dv">0</span>)[<span class="dv">0</span>]</span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>    ols <span class="op">=</span> LinearRegression()</span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(selected_features) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>        ols.fit(X_train[:, selected_features], y_train)</span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>        y_pred_ols <span class="op">=</span> ols.predict(X_test[:, selected_features])</span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a>        mse_ols_list.append(mean_squared_error(y_test, y_pred_ols))</span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a>        mse_ols_list.append(np.nan)  <span class="co"># Handle case where LASSO selects no features</span></span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a>    y_pred_lasso <span class="op">=</span> lasso.predict(X_test)</span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a>    mse_lasso_list.append(mean_squared_error(y_test, y_pred_lasso))</span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a>mean_mse_lasso <span class="op">=</span> np.nanmean(mse_lasso_list)</span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a>mean_mse_ols <span class="op">=</span> np.nanmean(mse_ols_list)</span>
<span id="cb46-38"><a href="#cb46-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-39"><a href="#cb46-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Average MSE of LASSO: '</span>, mean_mse_lasso)</span>
<span id="cb46-40"><a href="#cb46-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Average MSE of OLS: '</span>, mean_mse_ols)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Average MSE of LASSO:  445.7394808331279
Average MSE of OLS:  483.51659229493794</code></pre>
</div>
</div>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li>(KOR) Seunghwan Oh(?��?ȯ). (2019). Python Machine Learning Pandas Data Analysis(???̽? ?ӽŷ??? ?Ǵٽ? ?????ͺм?). ��????ȭ??</li>
<li>James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An introduction to statistical learning (1st ed.) [PDF]. Springer.</li>
</ul>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>